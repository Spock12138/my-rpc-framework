12-4:
依赖应该与 pom.xml 的 properties 同一个等级

为什么要叫 entity？因为这两个类是用来承载数据的实体对象

今日的任务：只要能看到客户端发出的 RpcRequest 对象在服务端控制台打印出来就算成功
具体实现需要分三步走
1. 让 rpc-core 认识 rpc-common
2. 编写服务端：在 rpc-core 里写一个能“听懂” Java 对象的 Netty 服务端。
3. 编写客户端：在 rpc-core (或者 test-client) 里写一个能“发送” Java 对象的 Netty 客户端。

第一步：打通模块依赖 (Module Dependencies)
你的 RpcRequest 在 rpc-common 模块里，而你的 Netty 代码在 rpc-core 里。 所以，你需要告诉 Maven：rpc-core 依赖 rpc-common。

第二步：编写服务端 (RpcServer)
我们在 rpc-core 模块下，新建包 com.github.spock12138.rpc.core.server，然后新建类 RpcServer。

总结 ---- 按照你的一步步操作，我已经实现了客户端和服务器端的通信。今天关于搭建 rpc 项目，包括创建项目框架，实现基础的客户端发出的对象，服务器端可以成功打印出来。创建的entity类包括 RpcRequest 与 rpcResponse，core 中创建了client 与 server 包，创建了 客户类与服务器类。


一、 服务端代码翻译 (RpcServer)
把服务端想象成一个繁忙的银行网点。
1. 两个线程组 (EventLoopGroup)
bossGroup（大堂经理）： 他只做一件事——站在门口**“迎宾”**（处理 accept 事件）。客人进门了，他就把客人领给柜台。
workerGroup（柜台职员）： 他们负责**“干活”**（处理 read/write 事件）。比如存钱、取钱、打印流水。
为什么分两个？ 为了效率。大堂经理不干杂活，专心迎宾，这样门口就不会排长队。
2. 启动助手 (ServerBootstrap)
这是一个配置单。因为 Netty 的组件太多了，用这个助手类来把所有的组件组装起来。
3. 流水线 (Pipeline & Handler) —— 全场核心
想象一条传送带。网络传过来的原本是 010101 这种字节流（包裹）。
ObjectDecoder（拆包员）： 把包裹拆开，把字节流变成一个 Java 对象（RpcRequest）。
SimpleChannelInboundHandler（业务员）： 拿到已经是对象的 RpcRequest，看里面的内容（打印日志）。
顺序至关重要： 必须先拆包（Decoder），业务员才能看懂。如果顺序反了，业务员拿到的就是一堆乱码。

二、 客户端代码翻译 (RpcClient)  --  寄快递
1. 启动与连接
客户端只需要一个线程组（因为不需要分工），负责拨打电话连接服务器。
2. 发送数据 (writeAndFlush)
Write（写）： 把信放到邮箱里。
Flush（冲刷）： 举起邮箱的小旗子，告诉邮递员“赶紧把信拿走发出去”。（如果不 Flush，数据可能会积压在本地缓冲发不出去）。
经过 Pipeline： 你的 RpcRequest 对象，会先经过 ObjectEncoder（打包员），被转化成字节流，然后通过网线发出去。

三、 总结：Netty 的“三板斧”
以后不管怎么改代码，核心逻辑永远是这三步：
老板与工人（Group）： 负责线程调度。
传送带（Pipeline）： 数据进来该怎么处理？（解码 -> 业务 -> 编码）。
大喇叭（ChannelHandler）： 具体的业务逻辑全写在这里（比如今天的打印日志，明天的调用服务）。


12-5:
遇到这个错误  Cannot resolve symbol 'HelloService'
test-server 模块根本不知道 rpc-api 模块的存在
虽然你在同一个项目里写了这两个模块，但在 Maven 的世界里，如果不在 pom.xml 里明确声明依赖，它们就是两个陌生人。
你需要去 test-server 模块的 pom.xml 里“介绍”它们认识。
解决方法
请打开 test-server 模块下的 pom.xml 文件，添加如下依赖代码

注意导包 (Imports) 鼠标放在红字上，按 Alt + Enter（或者 Option + Enter），选择 Import Class。

今天的总结复盘
一、 今天你到底学会了什么？（核心复盘）
1. 架构观：Maven 的“模块社交学”
遇到的坑： 你在写 HelloServiceImpl 时报错找不到 HelloService，这是新手最容易卡壳的地方。
学到的点： Maven 模块隔离。
默认情况下，test-server 根本不认识 rpc-api。你必须在 pom.xml 里显式添加 <dependency>，就像给两个陌生人发名片介绍认识一样。这是大型 Java 项目的标准管理方式。
2. 通信观：Netty 的“流水线思维”
代码体现： 你在 RpcClient 和 RpcServer 里写的 pipeline.addLast(...)。
学到的点： 责任链模式。
数据在网络传输时是 0/1 比特流。
解码器 (Decoder)：负责把“比特流”变成“Java 对象”（拆快递）。
业务 Handler：负责处理对象（看信件内容、回信）。
编码器 (Encoder)：负责把“Java 对象”变回“比特流”（打包发走）。
这种分工让代码非常清晰，以后你想换加密方式，只改编解码器就行，不用动业务逻辑。
3. 核心技：反射 (Reflection) 的“上帝视角”
代码体现： Server 端那几行 method.invoke(...)。
学到的点： 动态代理的基石。
普通的 Java 调用是写死的（service.sayHello()）。
RPC 的调用是动态的：“给我一个字符串 'sayHello'，我就能让它跑起来”。这就是框架和业务代码的区别——框架可以在不知道具体写了什么方法的情况下，去驱动业务代码运行。
二、 你的 RPC 运行全流程（脑内动画）
闭上眼睛，回想一下你刚才启动的那两个黑窗口，它们背后发生了这件事：
启动 (Server Start)： TestServer 启动，把一张“菜单”（HelloServiceImpl）交给了 RpcServer 保管（注册表 Map）。
打包 (Client Send)： RpcClient 启动，把你写的“我想调用 sayHello”这句话，封装进 RpcRequest 盒子，贴上快递单，通过 Netty 发出去。
解析与执行 (Server Work)：
Server 收到盒子，打开一看：“哦，要找 HelloService 的 sayHello 方法”。
Server 去查注册表，找到对应的对象。
Server 用 反射 手段，强行让那个对象执行了方法。
回信 (Response)： Server 把执行结果 “你好, Spock!” 装进 RpcResponse 盒子，原路发回。
收货 (Client Receive)： Client 收到盒子，拆开，打印在控制台上。

关于反射的总结
普通调用： obj.method()。像是按按钮。按钮是固定的，按这个按钮只能亮这盏灯。
反射调用： method.invoke(obj)。像是声控智能家居。你喊“开灯”，它解析你的指令去开灯；你喊“放音乐”，它解析指令去放音乐。RPC Server 就是那个听指令的智能音箱。



12-8:
今天的任务安排 ---- 把 Client 端的代码，用 动态代理 (Dynamic Proxy) 包装起来。让它看起来像在调用本地方法一样简单：helloService.sayHello("Spock")。

第一个遇到的就是梯子问题，设置在 IDEA 中设置 Git 代理
git config --global http.proxy http://127.0.0.1:7890
git config --global https.proxy http://127.0.0.1:7890
重试 Push：如果你以后关了代理软件，Git 可能会报错连不上。到时候在 Terminal 输入 git config --global --unset http.proxy 和 git config --global --unset https.proxy 取消设置即可。


核心概念：什么是“代理 (Proxy)”？
普通调用： 你直接给明星（Server）打电话。但明星在另一个城市（另一台电脑），你够不着。

代理模式： 你找明星的经纪人（Proxy）。
经纪人手里有一张和明星一模一样的名片（接口 Interface）。
你对经纪人说：“我要找明星唱歌（调用 sayHello）”。
经纪人说：“好，我帮你传达。”
经纪人默默地把你的请求写在纸上（封装 RpcRequest），寄给明星，等明星回信，然后再把结果告诉你。
你今天要写的 ClientProxy 就是这个经纪人。

第一步：改造 RpcClient (让它变成单纯的送信员)
昨天你的 RpcClient 里，RpcRequest 是硬编码写死的（Spock, sayHello）。 我们要把它改造成：给它什么信，它就送什么信。
修改 rpc-core 下的 RpcClient.java

第二步：创建动态代理类 (ClientProxy) —— 今天的核心
在 rpc-core 模块下，新建包 com.github.spock12138.rpc.core.proxy，新建类 ClientProxy

第三步：见证“本地调用”的时刻
我们去 test-client 模块，修改测试类 TestClient (或者你之前的 main 方法)。


你的“今日任务”清单
改造 RpcClient：把 request 改成参数传入。
新建 ClientProxy：复制上面的代码，理解 invoke 方法是怎么偷天换日的。
运行测试：
先启动 TestServer。
再启动新的 TestClient。
预期结果： 你虽然在 Client 端代码里只写了 helloService.sayHello(...)，但 Server 端应该能收到请求，并且 Client 端控制台会打印出 Server 发回来的响应。

今天学习包括的知识点 JDK 动态代理 + Netty 异步调用 + 反射

三、 完整的数据流向动画
当你执行 helloService.sayHello("Spock-Proxy") 时，数据是这样流动的：
TestClient 发起调用。
JDK 动态代理 捕获调用，直接跳转到 ClientProxy.invoke()。
ClientProxy 把 "sayHello" 和 "Spock-Proxy" 两个词，打包进 RpcRequest 对象。
ClientProxy 把 RpcRequest 对象递给 RpcClient。
RpcClient 通过 Netty 把对象发给 服务端。
(服务端处理...)
RpcClient 的 Handler 收到回信 "你好..."，并打印在控制台上。

明天的主线： 我们要让 TestClient 能真正拿到返回值，而不是去控制台看日志。 这就需要引入 CompletableFuture (异步转同步) 的机制。


一、 今日学习复盘：RPC 的“魔法时刻”
今天我们完成了从 “硬编码调用” 到 “动态代理调用” 的华丽转身。
1. 核心概念：动态代理 (Dynamic Proxy)
以前 (Day 1)：你需要手动造 RpcRequest 对象，像填表格一样一个个填（接口名、方法名...），非常麻烦且丑陋。
现在 (Day 2)：你只需要像调用本地方法一样调用接口 helloService.sayHello("...")。
原理：
我们利用 JDK 动态代理 (Proxy.newProxyInstance) 在内存中凭空生成了一个 HelloService 的**“傀儡对象”**。
这个傀儡没有任何业务逻辑，它的唯一作用就是把你的所有操作**“拦截”**下来，转发给 InvocationHandler（也就是我们写的 ClientProxy）。

2. 代码架构演进
我们修改了三个核心文件，它们的分工变得更加专业：
ClientProxy.java (新晋主角 - 经纪人)：
职责：实现了 InvocationHandler 接口。它是“拦截器”，负责将方法调用（Method Call）转换成数据对象（RpcRequest）。
关键点：它屏蔽了底层的网络细节，让调用者感觉不到网络的存在。
RpcClient.java (被改造 - 快递员)：
变化：从“只送特定包裹”变成了“通用快递员”。
职责：sendRequest(RpcRequest request)。给它什么请求，它就通过 Netty 发送什么请求。它不再关心业务逻辑。
TestClient.java (用户)：
变化：代码变得极度简洁。不再需要手动构建 Request，也不需要直接操作 Netty。
体验：clientProxy.getProxy(HelloService.class) 这一行代码，就是连接“本地”与“远程”的桥梁。



12-9:
RPC 项目中最核心、也是面试官最爱问的难点：异步转同步。

一、 核心逻辑：取餐号（Claim Check Pattern）
你现在的困境是：
Client (主线程)：把信扔进邮筒（Netty）就走了，没拿到回信。
Netty (子线程)：过了一会儿收到回信了，但它不知道这封信是谁寄的，也不知道该给谁。
解决方案：引入“取餐号”机制。
贴号 (Request ID)：主线程寄信前，在信封上写个唯一的号码（UUID），比如 "1001"。
存根 (Future)：主线程手里紧紧攥着一张写着 "1001" 的空白支票（CompletableFuture），坐在门口死等。
原样返回：服务端收到 "1001" 的信，处理完，在回信上也写上 "1001"。
兑现：Netty 收到 "1001" 的回信，大喊一声：“谁手里有 1001 的支票？”主线程听到了，把回信的内容填到支票上，任务完成。

二、 Step 1: 基础设施升级 (给信封加编号)
为了实现“对号入座”，Request 和 Response 必须带上 ID。
操作： 修改 rpc-common 模块下的 RpcRequest.java 和 RpcResponse.java，给它们都加上 requestId 字段。

三、 Step 2: 服务端配合 (抄写编号)
服务端必须很“懂事”，收到什么 ID，回信时必须把这个 ID 抄回去，否则客户端不认账。
操作： 修改 rpc-core 模块下 RpcServer.java 的 Handler 部分。

四、 Step 3: 客户端改造 (今天的重头戏)
我们要修改 rpc-core 下的 RpcClient.java。这里需要用到 CompletableFuture 和 ConcurrentHashMap。
逻辑预告 (注释驱动)：
// 1. 这是一个“全局信箱”，用来存放所有发出去了但还没收回来的请求。
//    Key 是请求 ID，Value 是那个“空白支票” (Future)。
private static final Map<String, CompletableFuture<RpcResponse>> PENDING_REQUESTS = new ConcurrentHashMap<>();

// 2. 发送逻辑 (sendRequest 方法)：
//    - 生成 UUID。
//    - 造一个 CompletableFuture，放进全局信箱。
//    - 发送请求。
//    - 阻塞等待 Future 有结果 (future.get)。

// 3. 接收逻辑 (Handler 的 channelRead0 方法)：
//    - 拿到响应里的 ID。
//    - 去全局信箱里把对应的 Future 找出来。
//    - 把响应塞进 Future 里 (future.complete)。

五、 Step 4: 修改 Proxy (把最后一块拼图补上)
现在 RpcClient.sendRequest 已经能返回 Object 结果了（不再是 null），我们需要修改 ClientProxy.java，把这个结果 return 出去。
修改 ClientProxy.java 的 invoke 方法：

今日任务： 按顺序执行这 4 步（Common -> Server -> Client -> Proxy）。 一旦 TestClient 能打印出结果，立刻 Commit + Push，然后结束今天的战斗！

今日复盘 (Summary)
今天我们解决的核心痛点是：Netty 发送是异步的（发完就走），但业务调用是同步的（必须等结果）。
我们引入了三个关键道具来解决这个问题：
唯一标识 (Request ID)：给每个请求贴上快递单号，确保回来的包裹能对上号。
全局信箱 (Map)：用来暂存那些“发出去但还没回来”的请求。
同步锁 (CompletableFuture)：让主线程在门口死等，直到结果回来。

你的“作弊条” (Cheat Sheet for Interview)
如果面试官问：“你的 RPC 怎么处理异步返回值的？” 标准答案：
“我使用了 CompletableFuture 结合 ConcurrentHashMap。
发送请求前，生成一个 UUID，创建一个 CompletableFuture 放入 Map 中，Key 是 UUID。
线程调用 future.get() 阻塞等待。
当 Netty 收到响应时，根据响应里的 UUID 从 Map 中取出 Future，调用 future.complete(response)。
这样主线程就被唤醒并拿到结果了。”



12-11:
开始今天的手写 rpc 项目，前几天完成了 异步转同步和动态代理。
// 这里的 IP 和端口是写死的！万一服务器换了 IP，或者我也想搞个集群，这就废了。
new ClientProxy("127.0.0.1", 9000);
今天的解决方案 —— 引入 Zookeeper： 我们要把 Zookeeper 当作一个**“房屋中介”**。
Server (房东)：启动时，把自己的地址（127.0.0.1:9000）贴到中介的墙上。
Client (租客)：启动时，不去直接找房东，而是先去中介墙上查：“HelloService 在哪里？”，拿到地址后再去连接
待会儿的动作：
我们需要引入一个新工具：Curator（它是 Java 操作 Zookeeper 的客户端库，就像 Jedis 之于 Redis）。
你需要在这个电脑上（或者 Docker 里）把 Zookeeper 跑起来。


今天我们只做一件事：让 Server 启动时，自动把自己的地址（IP:Port）写到 Zookeeper 上去。 这就是所谓的“服务注册”。
我们分两步走：定义接口 -> 实现 Zookeeper 逻辑。

第一步：定义注册中心接口 (ServiceRegistry)
为了以后能换成 Nacos 或 Redis，我们先定义一个标准接口。
在 rpc-core 模块下，新建包：com.github.spock12138.rpc.core.registry。
新建接口 ServiceRegistry：

第二步：实现 Zookeeper 注册逻辑 (ZkServiceRegistry)
这是今天的核心代码。我们要利用 Curator 客户端连接 ZK，并创建一个临时节点。
核心逻辑预告（注释驱动）：
连接 ZK：设置重试策略（连不上重试 3 次），启动客户端。
生成路径：比如 /my-rpc/com.xxx.HelloService/127.0.0.1:9000。
创建节点：必须是 EPHEMERAL (临时节点)。这样万一服务器挂了（断开连接），ZK 会自动把这个节点删掉，防止客户端调用到死掉的服务。
在同一个包下新建类 ZkServiceRegistry：


第三步：修改 RpcServer (让它启动时去注册)
最后一步，我们要让 RpcServer 启动的时候调用上面写好的代码。
修改 rpc-core 下的 RpcServer.java：
在类里增加一个成员变量 private final ServiceRegistry serviceRegistry;。
在构造函数里初始化它。
在 register 方法里，除了要把服务放进本地 Map，还要上报给 Zookeeper。

既然服务端已经把房子（地址）挂在 Zookeeper（中介）那了，现在我们要去改造客户端，让客户端去中介那查地址，而不是写死 127.0.0.1
要开始写 Client 端的“服务发现”逻辑了！


先总结刚刚的操作
新建接口 ServiceRegistry，实现 Zookeeper 注册逻辑 (ZkServiceRegistry)，修改 RpcServer (让它启动时去注册)
一、 宏观逻辑：为什么要搞这么复杂？
之前的状态（没有 Zookeeper）：
Server：默默地在 9000 端口启动，谁也不告诉。
Client：必须靠猜（或者写死代码）127.0.0.1:9000 才能找到 Server。
痛点：Server 换个 IP，Client 就瞎了。
现在的状态（有了 Zookeeper）：
Zookeeper 就是一个**“房产中介”**（或者公告栏）。
动作 1： Server 启动时，主动跑去中介那里登记：“我是 HelloService，我的房源地址是 127.0.0.1:9000”。
动作 2（这就是刚才代码做的事）： 中介在墙上贴了一张条子（创建了一个节点）。
二、 微观拆解：刚才那三个文件在干嘛？
你刚才修改了三个文件，它们就像是一个**“分工明确的小团队”**。

1. ServiceRegistry.java (接口) —— 定规矩的
角色：它是**“标准制定者”**。
逻辑：它说：“不管你是用 Zookeeper、Nacos 还是 Redis 做中介，反正你必须得能干这件事——注册（register）”。
意义：为了以后你想换 Nacos 时，不需要改动核心业务代码，只需要换个实现类就行（面向接口编程）。
2. ZkServiceRegistry.java (实现类) —— 真正干活的
角色：它是**“Zookeeper 办事专员”**。
逻辑：
构造函数：它负责给 Zookeeper 打电话（建立连接），还要处理断线重连（RetryPolicy）。
register 方法：它负责把 Server 的信息写成 Zookeeper 能看懂的格式。
路径：/my-rpc/com.xxx.HelloService/127.0.0.1:9000
类型：临时节点 (Ephemeral)。这非常关键！
为什么？ 如果 Server 挂了（断气了），它肯定没法通知 Zookeeper 删数据。但因为是临时节点，Zookeeper 发现连接断了，就会自动撕掉这张房源条子。这样 Client 就不会再去连一个死掉的服务了。
3. RpcServer.java (调用者) —— 发号施令的
角色：它是**“Server 老板”**。
逻辑：
以前：它只知道把自己存到本地 Map 里（为了反射调用）。
现在：它在启动的时候（register 方法里），顺手把自己的地址扔给了办事专员（serviceRegistry），说：“去，把我的地址通过 Zookeeper 广播出去。”

四、 你的决定
现在你明白了吗？
我们不是在为了写代码而写代码。
我们是为了在 Zookeeper 上留下这个节点。
接下来我们要做的（Client 端），就是去“读取”这个节点


Client 接下来的动作很简单：
进店（连接 ZK）：Client 也得连上 Zookeeper。
查房（Service Discovery）：Client 问：“我想找 HelloService 的房源”，Zookeeper 返回一个列表（比如 ["127.0.0.1:9000"]）。
入住（连接 Server）：Client 拿到地址，直接去连 Server。
我们依然分三步走：定义接口 -> 实现逻辑 -> 改造代理。

第一步：定义发现接口 (ServiceDiscovery)
和注册一样，我们先定个规矩。
在 rpc-core 的 com.github.spock12138.rpc.core.registry 包下，新建接口 ServiceDiscovery

第二步：实现 Zookeeper 查房逻辑 (ZkServiceDiscovery)
这是 Client 端的 ZK 专员。它的核心任务是 getChildren（获取子节点列表）。
核心逻辑预告：
路径：/my-rpc/服务名。
获取列表：调用 client.getChildren().forPath(path)，它会返回该服务下所有可用的地址列表（比如 ["127.0.0.1:9000", "192.168.1.5:9000"]）。
负载均衡（简版）：今天我们先不做复杂的轮询或随机，直接拿列表里的第一个（get(0)）。明天我们专门讲怎么在这里做负载均衡。

第三步：改造经纪人 (ClientProxy)
这是最后一步。我们要让 ClientProxy 变聪明。
以前：它脑子里只记得一个死地址（构造函数传进来的）。
现在：它手里拿着一本黄页（ServiceDiscovery），每次发请求前，先查一下地址。
修改 rpc-core 下的 ClientProxy.java：
把成员变量 host 和 port 删掉。
换成 ServiceDiscovery。
在 invoke 方法里，动态查找地址。


一、 核心复盘：Client 端到底发生了什么？
刚才的三步操作，其实就是教会客户端**“查电话本”**的过程。
1. ServiceDiscovery (接口) —— 定规矩
之前的你：直接把电话号码（IP:Port）背下来，写在 ClientProxy 里。
现在的你：意识到脑子记不住那么多号码，决定买一本“电话簿”。
代码含义：这个接口就是“电话簿”的标准——不管它是纸质的（Zookeeper）还是电子的（Nacos），它必须具备**“查号 (lookup)”**的功能。
2. ZkServiceDiscovery (实现类) —— 查电话簿的动作
这是你刚才觉得最复杂的代码。试着别看代码，闭上眼想一下逻辑：
第一步：翻开电话簿（连接 Zookeeper）。
第二步：找到“HelloService”那一页（路径 /my-rpc/com.xxx.HelloService）。
第三步：看上面贴了哪些号码（client.getChildren() 获取子节点列表 ["127.0.0.1:9000"]）。
第四步：随便挑一个号码抄下来（list.get(0)）。

3. ClientProxy (代理) —— 打电话的人
这是变化最大的地方。
以前：
new RpcClient("127.0.0.1", 9000) -> 直接拨号。
现在：
discovery.lookupService("HelloService") -> 先查号。
拿到 address。
new RpcClient(address) -> 再拨号。


在编程领域，先看懂“我要做什么”（宏观逻辑），再看懂“这块代码负责哪一部分”（代码块逻辑），最后才去纠结“这行代码为什么这么写”（语法细节），是最不容易迷路的方法。
第一部分：服务端去登记 (ZkServiceRegistry.java)
这个类的逻辑其实就两块：“连上中介” 和 “贴个条子”。
📌 代码块 1：建立连接 (构造函数)
逻辑：就像你去房产中介，得先走进店门，跟工作人员打个招呼。
关键动作：
告诉代码 Zookeeper 在哪 (127.0.0.1:2181)。
设置一个“命名空间” (namespace = "my-rpc")。
解释：相当于告诉中介：“我不租房，我是来发布‘RPC服务’的”。以后所有的记录都会归档在这个目录下，防止跟别人的数据混了。
启动客户端 (client.start())。
📌 代码块 2：注册服务 (register 方法)
逻辑：这是最核心的一步。你要在墙上贴一张纸条。
关键动作：
拼路径：把服务名和地址拼成一个路径字符串。
比如：/com.xxx.HelloService/127.0.0.1:9000。
创建节点：告诉 Zookeeper 创建这个路径。
重点细节：这里设置了**“临时节点” (Ephemeral)**。
含义：这就像你跟中介说：“如果我电话打不通了（断连了），就把这张纸条撕了，别让人白跑一趟。”这是自动下线的关键。

第二部分：客户端去查号 (ZkServiceDiscovery.java)
这个类的逻辑也只有两块：“问中介” 和 “挑一个”。
📌 代码块 1：获取列表 (lookupService 前半部分)
逻辑：你问中介：“HelloService 这个服务，现在有哪些人提供？”
关键动作：
拼父路径：/com.xxx.HelloService。
查子节点 (client.getChildren())：Zookeeper 会去这个目录下看一眼，返回所有贴在那里的条子。
返回结果是一个列表 List，比如 ["127.0.0.1:9000", "192.168.1.5:9000"]。
📌 代码块 2：负载均衡 (lookupService 后半部分)
逻辑：中介给了你 5 个地址，你得选一个去连。
关键动作：
判空：先看看列表是不是空的，要是没人提供服务，直接报错。
选择：我们现在的代码很简单，直接 list.get(0)，拿第一个。
未来扩展：明天我们这里会改成“随机选”或者“轮询选”，这就是负载均衡。

第三部分：代理人去拨号 (ClientProxy.java)
这是把上面连起来的地方。
📌 代码块 1：拦截与查找 (invoke 方法前半部分)
逻辑：当你调用 sayHello 时，代理人先把它拦下来。它心里想：“别急着连，我得先去问问地址变了没。”
关键动作：
拿到接口名 (HelloService)。
调用上面的 discovery.lookupService() 方法。
拿到一个活着的、真实的 IP 地址。
📌 代码块 2：连接与发送 (invoke 方法后半部分)
逻辑：拿到真实地址后，剩下的事就和以前一样了。
关键动作：
动态连接：用刚才查到的 IP，现场创建一个 RpcClient。
发送请求：把数据发过去。

总结一下现在的流程
如果把整个流程拍成一部电影，分镜脚本是这样的：
服务端启动：ZkServiceRegistry 跑到 Zookeeper 那里，创建了一个临时节点 /HelloService/127.0.0.1:9000。
客户端调用：你代码里写了 helloService.sayHello()。
代理拦截：ClientProxy 暂停动作，转身去问 ZkServiceDiscovery。
发现服务：ZkServiceDiscovery 问 Zookeeper：“HelloService 在哪？”，拿到 127.0.0.1:9000。
正式连接：ClientProxy 拿着这个地址，建立 Netty 连接，发送数据。


1. 今天的成就：   ⭐⭐⭐ 总结
环境搭建：用 Docker 一行命令跑起了 Zookeeper，从此不再怕环境配置。
核心逻辑：
服务端：学会了“贴小广告”（ZkServiceRegistry），Server 启动自动上报地址。
客户端：学会了“查电话本”（ZkServiceDiscovery），不再写死 IP，而是动态发现服务。
代理层：改造了 ClientProxy，让它从“直连模式”升级为“智能路由模式”。
八股文储备：复习了 Java 并发（锁、线程状态、线程池）和 Spring 核心原理。
2. 明日预告 (Day 5)：
你现在的客户端虽然能查到地址，但它是傻傻地拿第一个 (get(0))。
明天我们要实现真正的 负载均衡 (Load Balance)：
如果有 3 个服务器，怎么轮着调用？(轮询)
怎么随机调用？(随机)
怎么保证同一个用户总是调同一个服务器？(一致性 Hash)



12-14:
接下来实现负载均衡。
1.为什么要搞负载均衡？
今天的目标： 我们要写一个**“分配器”**，根据特定的规则（随机、轮询等），从这 3 个地址里选出一个，让大家的活儿干得均匀一点。

2. 它的位置在哪里？(The Where)
  负载均衡是夹在 “服务发现” 和 “建立连接” 中间的一层。
  以前的流程： ServiceDiscovery (给一堆地址) --> 直接拿第一个 --> RpcClient (连接)
  今天的流程： ServiceDiscovery (给一堆地址) --> 【LoadBalancer (选一个)】 --> RpcClient (连接)

03. 接下来怎么做？(The How - 三步走)
为了让你的 RPC 框架既灵活又专业，我们将采用策略模式 (Strategy Pattern)。
第一步：定规矩 (Interface)
我们不能把选择逻辑写死。我们要定义一个接口 LoadBalancer。
输入：一堆地址列表 (List<String>)。
输出：被选中的那个地址 (String)。
第二步：写策略 (Implementations)
我们会实现两个最经典的策略：
随机 (Random)：闭着眼睛瞎指一个。实现最简单，在大量请求下其实很均匀。
轮询 (RoundRobin)：排排坐，吃果果。第一次给 A，第二次给 B，第三次给 C，第四次又回 A。
第三步：装进代理 (Integration)
回到 ClientProxy 或 ZkServiceDiscovery，把原来那个丑陋的 get(0) 删掉，换成我们的 loadBalancer.select(list)。


docker start zookeeper  --  启动 zookeeper



12-15:
启动 testServer 报错，具体信息如下 org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /my-rpc/com.github.spock12138.rpc.api.HelloService/127.0.0.1:9001
原因：Zookeeper 的“幽灵节点”。
看你的截图，报错是 NodeExists ... 127.0.0.1:9001。
发生了什么：
你之前可能非正常关闭了 9001 端口的 Server（比如直接点了红方块停止，或者程序崩了）。
虽然 Server 死了，但 Zookeeper 还没反应过来（默认有 40秒 的 Session 超时时间），它以为 Server 还活着，所以那个临时节点还在。
当你立马再次启动 9001 时，代码试图创建同一个节点，ZK 就会喊：“报错！这个节点已经有了！”。

解决方法：代码里做一个“宽容处理”
我们需要修改 ZkServiceRegistry.java，在注册前先检查一下，或者捕获这个异常把它吞掉（因为既然节点存在，说明已经注册过了，或者需要覆盖）。
更稳健的写法（先删后建）：
请修改 ZkServiceRegistry.java 的 register 方法：

// 【新增】如果节点已经存在，先删除它（防止之前非正常关闭残留）
        if (client.checkExists().forPath(servicePath) != null) {
            System.out.println("发现旧节点，正在删除: " + servicePath);
            client.delete().forPath(servicePath);
        }

今天遇到的问题 -- JVM 进程状态隔离 和 Zookeeper 的节点残留。
问题一：为什么轮询失效（总是连 9000）？
原因：你“重置”了计数器。
你的代码逻辑：RoundRobinLoadBalancer 里有一个 AtomicInteger index = 0。
你的操作：每次点 IDEA 的“运行”按钮，都是启动一个新的 Java 进程（JVM）。
后果：
第一次运行 TestClient：新 JVM -> 新 LoadBalancer -> index 初始化为 0 -> 选第 0 个 (9000)。程序结束，内存清空。
第二次运行 TestClient：又是新 JVM -> 又是新 LoadBalancer -> index 还是初始化为 0 -> 选第 0 个 (9000)。
结论：因为你每次都重启了客户端，计数器根本没机会加到 1。
✅ 解决方法：在一次运行中循环调用
请修改 TestClient.java，让它在一个 main 方法里连续调用 5 次。这样计数器才能累加。


1. 为什么“重启”也能解决报错？(原理分析)
你提到：“关闭报错的节点，然后重启，第二次就不存在错误了”。
这其实验证了 Zookeeper “临时节点 (Ephemeral Node)” 的核心特性：
生命周期绑定会话：当你的 Java 程序（Server 9001）非正常退出时，Zookeeper Server 不会立刻知道（它以为网络卡顿了）。
超时机制：过了几十秒（Session Timeout），Zookeeper 发现：“哎，这个 9001 怎么还不给我发心跳？肯定挂了。” 于是它自动删除了那个节点。
结果：等你下次启动时，节点已经被删了，所以不报错了。
那为什么我们要加代码修复？ 因为在生产环境或者高频调试时，我们往往需要在服务挂掉后的 1秒内 立刻重启。这时候 Zookeeper 还没来得及删旧节点，新服务就会启动失败。你后来加的 checkExists 代码，就是为了应对这种**“极速重启”**的场景。

2. 这个阶段的里程碑意义
看着这张截图，你现在拥有的已经是一个真正意义上的微服务架构雏形：
多实例部署：你同时跑了两个 Server。
服务注册与发现：Zookeeper 自动维护了这份名单。
客户端负载均衡：Client 端自己决定把流量打给谁，而且做到了雨露均沾。


今天的任务总结：
不仅仅是“实现了轮询”，更重要的是你引入了**“策略模式”**，搭建了一个可扩展的架构。你对 Java 语法的那个困惑点，其实正是 Java 面向对象最核心的概念——多态 (Polymorphism)。
第一部分：解惑 Java 语法 —— “多态”与“向上转型”
你的困惑在于：为什么左边声明的是 LoadBalancer（接口），右边 new 的却是 RandomLoadBalancer 或 RoundRobinLoadBalancer（实现类）？
这在 Java 里叫做 “向上转型” (Upcasting)，或者更通俗地说，是 “父类引用指向子类对象”。
1. 通俗比喻：USB 接口
LoadBalancer (接口)：就像电脑上的 USB 插口。标准规定：只要插进来的东西，必须能“传输数据”（对应 select 方法）。
RandomLoadBalancer (实现类)：就像 鼠标。它是 USB 设备的一种。
RoundRobinLoadBalancer (实现类)：就像 键盘。它也是 USB 设备的一种。
语法逻辑： 电脑（ZkServiceDiscovery）说：“我需要一个 USB 设备（LoadBalancer）。” 你可以给它插个鼠标（new Random...），也可以插个键盘（new RoundRobin...）。 电脑不在乎具体插的是啥，它只知道“这个东西肯定能传输数据（能调用 select）”。
2. 代码层面的好处
如果不这么写，你的代码就死了：
面向接口编程



12-16:
今天的 rpc 项目任务 自定义序列化 (Custom Serialization)
你下午将会面临的挑战（预告）：

告别 Java 原生：
目前你应该用的是 ObjectOutputStream / ObjectInputStream（JDK 自带）。
缺点：生成的二进制流太大，序列化/反序列化速度太慢，且无法跨语言。

引入 Kryo (或者 Protobuf/Hessian)：
Kryo 是 Java 生态里最快的序列化框架之一，非常适合 RPC。
你需要引入 Kryo 依赖，写一个序列化工具类。

Netty 管道改造 (Handler)：
这是最容易晕的地方。你需要把你现在的 CommonEncoder 和 CommonDecoder（或者你用的 ObjectEncoder）替换掉。

重点逻辑：
编码 (Encoder)：把 Java 对象（RpcRequest） -> 转换成 -> 字节数组 (byte[]) -> 写入 ByteBuf。
解码 (Decoder)：从 ByteBuf 读取字节 -> 转换成 -> Java 对象。

解决“粘包/半包”问题：
既然不用 JDK 自带的流了，你就得自己定义协议格式（魔数 + 长度 + 序列化类型 + 数据）。
剧透：你需要写一个自定义协议，这会非常有意思！


拓展任务 实现 一致性哈希。
它的核心场景不匹配：一致性 Hash (Consistent Hash) 最主要的作用是解决 “有状态”服务 的缓存抖动问题（比如 Redis 集群扩容，或者分布式 Session）。
RPC 的现状：你的 RPC Server 目前是 “无状态” (Stateless) 的。
Client 无论连上 Server A 还是 Server B，调用的 helloService.sayHello() 逻辑是一模一样的，结果也一样。
这种情况下，轮询和随机其实是最好、最高效的。
开发成本 vs 收益：实现一致性 Hash 需要写 Hash 环（TreeMap）、虚拟节点等逻辑，代码量不小，但对你目前的 RPC 演示效果提升几乎为零。

负载均衡可以视为反向代理的一种应用，负载均衡的方法大致可以分为传统负载均衡算法和哈希算法两种
反向代理 reverse proxy 是指以代理服务器来接收由客户端发送来的请求，并通过一定的策略将其转变发给实际处理请求的后端服务器
1 传统负载均衡算法
2 Mod-N 哈希
3 一致性哈希 Consistent Hashing



为什么需要自定义序列化？
JDK 原生序列化：把家里的房子、地基、甚至门牌号全都打包进去，导致包裹巨大无比，运费（网络带宽）极贵，拆快递（反序列化）还慢。
自定义序列化 (如 Kryo/Protobuf)：只把家具和衣服拿出来，压缩打包。包裹小、运费便宜、拆包极快。
主要有 3 个痛点 逼着我们必须换掉 JDK 原生序列化：

1. 效率太低 (太慢 & 太大) —— 核心原因
体积大 (Bloat)：JDK 序列化会在二进制流里塞入大量的元数据（比如完整的类名、包名、继承关系、serialVersionUID 等）。
测试数据：同一个对象，JDK 序列化后的大小可能是 Kryo 的 5-10 倍。
速度慢：JDK 底层严重依赖反射，也没有针对性的缓存优化，编解码速度很慢。在 RPC 这种高并发场景下，它会成为吞吐量的瓶颈。
2. 跨语言支持差 (死板)
JDK 序列化：是 Java 内部的“方言”。
如果你的 Server 是 Java 写的，Client 是 Python 或 Go 写的，它们根本听不懂 JDK 序列化的二进制流。
自定义 (如 JSON/Protobuf)：是“普通话”。大家都能读懂。
3. 安全性问题 (漏洞)
JDK 序列化存在著名的反序列化漏洞。黑客可以构造一个恶意的二进制流，只要你的服务器反序列化它，就可能被远程执行恶意代码（RCE）。这在生产环境中是致命的。



完成这三步操作（改 pom，写接口，写实现类）。
写完后，我们手里就有了一把“屠龙刀”（Kryo）。但现在的 RPC 还在用“水果刀”（JDK流）。 下一步，我们要去修改 Netty 的 Encoder 和 Decoder，把这把屠龙刀装上去！
问题：Kryo 这个对象本身是非线程安全的。
想象 Kryo 是一个“写字板”。
如果是单例（static），线程 A 正在往上写数据，线程 B 突然跑过来擦掉了一半写自己的，数据就乱套了。
解决：ThreadLocal 就像是给每个线程发了一个专用的写字板。
线程 A 用自己的，线程 B 用自己的，互不干扰。
配置项解释：
setRegistrationRequired(false)：这很重要。默认 Kryo 要求你把每一个要传输的类（如 RpcRequest）都提前注册给它。关掉这个选项，它就会自动把类名写入序列化数据里，虽然体积稍大一点点，但极大地减少了开发麻烦（不然你每加一个类都要来改代码注册）。

为什么不能直接发字节？(粘包/半包问题)
TCP 是一个“流”协议，就像水流一样，没有边界。 如果你连续发了两个包 A 和 B：
粘包：Server 可能一次性收到了 AB 粘在一起的数据。
半包：Server 可能先收到 A 的一半，过一会儿才收到 A 的另一半。
为了解决这个问题，我们需要设计一个**“协议格式”**，就像快递单一样，明确告诉 Server：
魔数 (Magic Number)：验证这是不是我们的包（防止别人乱发数据）。
序列化器编号：告诉 Server 用什么工具解包（1=Kryo, 2=JSON...）。
数据长度：告诉 Server 这个包有多长（解决粘包问题）。
数据内容：真正的对象数据。

两个类其实是在实现一个自定义的应用层协议。我们不直接发裸数据，而是像“寄快递”一样，给数据打上标准的包装
1. 编码器 (CommonEncoder) —— “专业的打包员”
核心任务：把 Java 对象（RpcRequest）变成二进制流（ByteBuf），并贴上快递单。
这一步比较简单，就是按顺序写字节。
2. 解码器 (CommonDecoder) —— “谨慎的拆包员”
核心任务：从连绵不断的字节流中，精准地切分出一个个完整的包，然后还原成对象。
这是 RPC 网络层最难理解的部分。Netty 可能会分多次把数据发给你，也可能一次给你发好几个包。解码器必须像一个守门员，数据不够坚决不放行。

总结一下 Netty 解码器的“回退机制”
你可以把它想象成在自助餐厅排队取餐：
Mark: 你站在起点，记住了自己的位置。
Read: 你看看餐盘里的菜（读取 Header 里的 length）。
Check: 只有 3 个鸡腿，但你需要 10 个（readableBytes < length）。
Reset: 你不能拿走这 3 个鸡腿，必须退回到起点（resetReaderIndex），等后厨补齐了 10 个鸡腿，你再从头开始拿。
如果不回退（Reset），那 3 个鸡腿就被你“吃掉”了（字节被读丢了），等剩下的 7 个来的时候，就拼不回原来的样子了。






















